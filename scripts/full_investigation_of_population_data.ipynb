{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5fbda5c-7dc8-4f41-943a-b3b32b12f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regionmask\n",
    "import xarray as xr\n",
    "import h5netcdf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as patches\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "from cartopy.feature import NaturalEarthFeature\n",
    "from matplotlib import colors\n",
    "from functools import reduce\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ca7c9-4d60-4e58-bb69-f6abdc43ac13",
   "metadata": {},
   "source": [
    "# Investigation into individual countries time series\n",
    "The continuity of a time series is important because it ensures that the data accurately represents the underlying phenomenon/process being measured or observed (in our case, the grid/country/global population or gdp).\n",
    "A lack of continuity can introduce biases, distortions, or inaccuracies in the analysis and interpretation of the data.\n",
    "\n",
    "In the following, we showcase several cases where the population/gdp data used in ISIMIP2 lacks in continuity.\n",
    "We begin our assessment with country level time series.\n",
    "Once we understand the issue, we can design a correction algorithm to adress it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c17c14-2e26-4bcc-b1b0-735c438346e8",
   "metadata": {},
   "source": [
    "## Load the raw data and define the objects which will support the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "773592c6-d05e-4416-b7e8-239cfc67c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pop data from 1861 till 2100\n",
    "hist_data = xr.open_dataset(\"../data/population_histsoc_rcp26soc_0p5deg_annual_1861-2100.nc4\", engine='h5netcdf', decode_times=False).pop[:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9921d35d-8861-4884-8c64-c462fd8ed3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GeoAxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the regions over which future masking will be applied\n",
    "# In our case we are interested in country level aggregation\n",
    "countries = regionmask.defined_regions.natural_earth_v5_0_0.countries_110\n",
    "countries.plot(add_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d056757-a60e-460a-a7fd-77063fd398f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a mask object for each country based on the grid of our input data\n",
    "# This will result in an object that for a given country will have all corresponding gridcells filled with the country index value\n",
    "# For example for US all corresponding gridcells will be filled with \"4\"\n",
    "# All gridcells which are not part of any country will be filled with NaN\n",
    "mask = countries.mask(hist_data[0,:,:]).fillna(177) # we fillna values with a custom key \"177\" because some of these points, not identified as countries due to limits of regionmask package, actually contain some population (e.g., an insular country or not attributed coastline gridcells) and using this key we can still have access to these points\n",
    "\n",
    "# Obtain a list of all countries names and index:\n",
    "countries_names = countries.names\n",
    "countries_index = countries.numbers\n",
    "\n",
    "# Create a fictive country which would contain all the 177 masked gridcells\n",
    "countries_names.append(\"Not a regionmask country\")\n",
    "countries_index.append(177)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e957866-0d95-46f2-a8cf-6d9867b25889",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compute individual countries time series and plot these time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c79fb566-09e4-4c56-8b86-b4bae2a9eb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year completed: 1880\n",
      "Year completed: 1900\n",
      "Year completed: 1920\n",
      "Year completed: 1940\n",
      "Year completed: 1960\n",
      "Year completed: 1980\n",
      "Year completed: 2000\n",
      "Year completed: 2020\n",
      "Year completed: 2040\n",
      "Year completed: 2060\n",
      "Year completed: 2080\n",
      "Year completed: 2100\n"
     ]
    }
   ],
   "source": [
    "# Initialize a zero array with countries and time dimensions\n",
    "country_pop_timeseries = xr.DataArray(np.zeros((178, 240)), dims=('country', 'time'), \n",
    "                                      coords={'country': countries_index, 'time': range(1861, 2101)})\n",
    "\n",
    "# Loop over years:\n",
    "for year in range(1861, 2101):\n",
    "    # Loop over each countries:\n",
    "    for country in range(len(countries_index)):\n",
    "        # Calculate country total population for given year\n",
    "        country_pop_timeseries[country, year-1861] = np.nansum(hist_data[year-1861].where(mask == country))\n",
    "    \n",
    "    if year%20 == 0:\n",
    "        print(\"Year completed: \" + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a99508-baee-4667-9619-e90000ee43c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming time series starts from 1861 and ends in 2100\n",
    "years = pd.date_range(start='1861', periods=240, freq='Y').year\n",
    "\n",
    "# Iterate over each country and its population time series\n",
    "for i, country in enumerate(countries_names):\n",
    "    pop_timeseries_original = country_pop_timeseries[i]  # Original data, no changes\n",
    "    \n",
    "    # Creating a DataFrame for easy manipulation\n",
    "    df = pd.DataFrame({'Year': years, 'Population_original': pop_timeseries_original})\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['Year'], df['Population_original'], 'ro', markersize=3)\n",
    "    plt.axvline(x=2005, color='green', linestyle='--')  # This line adds a vertical line at the year 2005\n",
    "    plt.axvline(x=2010, color='green', linestyle='--')  # This line adds a vertical line at the year 2010\n",
    "    plt.title(country)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Population')\n",
    "    \n",
    "    # Save to pdf\n",
    "    pdf_path = f\"../results_to_share/{country}_original_pop_time_series_1861_2100.pdf\"\n",
    "    plt.savefig(pdf_path, format='pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7918a16-e01f-4ae9-852a-d89f60821566",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Interpretation of the time series discontinuities:\n",
    "By looking into each country time series, we can identify the following situation:\n",
    "1) Some of the time series are almost perfectly continuous (e.g. Rwanda, Uganda, Ethiopia)\n",
    "2) There is a big jump or drop in some countries' population time series between 2005 and 2010. Instead of bias adjusting the future or historic time series, it seems that the dataset was merged by performing a linear interpolation between 2005-2010. \n",
    "\n",
    "While this seems to work in some cases, there are many instances where this causes unrealistic behavior. Such examples include: sudden drop, followed by sudden increase (e.g. S. Sudan), a big jump or drop between the affected time period (e.g. Djibouti, Somaliland), or stagnation followed by change again (e.g. for Not a regionmask fictive country)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c18fd4-3286-4ea1-9527-7ab3fe818bb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Solution: fix countries time series for population\n",
    "Since for some countries, the original dataset seems perfectly fine, the correction algorithm is applied only for countries with visible anomalies.\n",
    "The list of countries for which correction is needed is constructed by visually inspecting the original time series produced in the earlier step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3bccc3b-e114-484c-a0df-936fd24d0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the countries for which the correction algorithm will be applied:\n",
    "list_of_countries_to_correct = ['Fiji',  'W. Sahara',  'Bahamas',  'Falkland Is.',  'Greenland',  'Lesotho',  'Belize',  'Jamaica',  'Botswana',  'Central African Rep.',  'Congo',  'Gabon',  'Lebanon',  'Palestine',  'Tunisia',  'Vanuatu',  'Bhutan', 'Kyrgyzstan', 'Turkmenistan',  'Armenia',  'Latvia',  'Estonia', 'Albania',  'Croatia',  'Ireland',  'New Caledonia',  'Solomon Is.',  'Sri Lanka', 'Taiwan', 'Italy',  'Iceland',  'Azerbaijan',  'Brunei',  'Slovenia',  'Czechia',  'Paraguay',  'N. Cyprus',  'Cyprus',  'Morocco', 'Djibouti', 'Somaliland', 'North Macedonia', 'Montenegro', 'Kosovo', 'Trinidad and Tobago', 'S. Sudan', 'Not a regionmask country']\n",
    "# create a dictionary mapping country names to their indices\n",
    "country_to_index = {name: index for index, name in enumerate(countries_names)}\n",
    "\n",
    "# find the indices of the countries to be corrected\n",
    "indices_to_correct = [country_to_index[country] for country in list_of_countries_to_correct if country in country_to_index]\n",
    "\n",
    "# Store the corrected national population time series\n",
    "corrected_country_pop_timeseries = country_pop_timeseries.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4f7400b-4aed-4846-8c41-1402cf198339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming time series starts from 1861 and ends in 2100\n",
    "years = pd.date_range(start='1861', periods=240, freq='Y').year\n",
    "\n",
    "# Determine index of years of interest\n",
    "index_2003 = list(years).index(2003)\n",
    "index_2005 = list(years).index(2005)\n",
    "index_2006 = list(years).index(2006)\n",
    "index_2010 = list(years).index(2010)\n",
    "index_2011 = list(years).index(2011)\n",
    "index_2015 = list(years).index(2015)\n",
    "\n",
    "# Iterate over each country and its population time series\n",
    "for i, country in enumerate(countries_names):\n",
    "    pop_timeseries_original = country_pop_timeseries[i]  # Original data, no changes\n",
    "    pop_timeseries_corrected = country_pop_timeseries[i].copy(deep=True)  # Corrected data, we will change this\n",
    "        \n",
    "    # Make the slope transition smoother between 2006-2011 if country in the list to be corrected\n",
    "    if country in list_of_countries_to_correct:\n",
    "        # Calculate the difference between years 2005 and 2006\n",
    "        diff_2005_2006 = pop_timeseries_corrected[index_2006] - pop_timeseries_corrected[index_2005]\n",
    "\n",
    "        # Calculate the average annual change in population over 2003-2005\n",
    "        avg_annual_change_between_2005_and_2003 = (pop_timeseries_corrected[index_2005] - pop_timeseries_corrected[index_2003]) / 3.0\n",
    "\n",
    "        # Calculate the average annual change in population over 2011-2015\n",
    "        avg_annual_change_between_2015_and_2011 = (pop_timeseries_corrected[index_2015] - pop_timeseries_corrected[index_2011]) / 5.0\n",
    "\n",
    "        # Create an array with increments transitioning from avg_annual_change_between_2005_and_2003 to avg_annual_change_between_2015_and_2011\n",
    "        # This allows for a smooth transition, with the slopes gradually increasing/decreasing to properly transition between the period 2006-2011\n",
    "        increment_values = np.linspace(avg_annual_change_between_2005_and_2003, avg_annual_change_between_2015_and_2011, index_2011 - index_2006)\n",
    "        for j in range(index_2006, index_2011):\n",
    "            pop_timeseries_corrected[j] = pop_timeseries_corrected[j-1] + increment_values[j-index_2006]\n",
    "\n",
    "    # Calculate the difference between years 2010 and 2011\n",
    "    diff_2010_2011 = pop_timeseries_corrected[index_2011] - pop_timeseries_corrected[index_2010]\n",
    "\n",
    "    # Adjust the population in 2011 and afterward if country in the list to be corrected\n",
    "    if country in list_of_countries_to_correct:\n",
    "        pop_timeseries_corrected[index_2011:] = pop_timeseries_corrected[index_2011:] + avg_annual_change_between_2015_and_2011 - diff_2010_2011\n",
    "        \n",
    "    \n",
    "    corrected_country_pop_timeseries[i] = pop_timeseries_corrected\n",
    "    \n",
    "    df = pd.DataFrame({'Year': years, 'Population_original': pop_timeseries_original, 'Population_corrected': pop_timeseries_corrected})\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['Year'], df['Population_original'], 'ro', markersize=3, label='Original Population')\n",
    "    plt.plot(df['Year'], df['Population_corrected'], 'k-', label='Corrected Population')\n",
    "    plt.axvline(x=2005, color='green', linestyle='--')  # This line adds a vertical line at the year 2005\n",
    "    plt.axvline(x=2010, color='green', linestyle='--')  # This line adds a vertical line at the year 2005\n",
    "    plt.title(country)\n",
    "    # Add text box with \"Corrected\" or not label\n",
    "    if country in list_of_countries_to_correct:\n",
    "        plt.text(0.1, 0.6, 'Corrected', transform=plt.gca().transAxes, fontsize=14,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    else:\n",
    "        plt.text(0.1, 0.6, 'No correction needed', transform=plt.gca().transAxes, fontsize=14,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Population')\n",
    "    \n",
    "    # Save to pdf\n",
    "    pdf_path = f\"../results_to_share/{country}_corrected_pop_time_series_1861_2100.pdf\"\n",
    "    plt.savefig(pdf_path, format='pdf')\n",
    "    \n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d19d7e-d194-4b92-841b-8b96fff5a0b0",
   "metadata": {},
   "source": [
    "# Downscale the corrected countries' population time series\n",
    "After correcting the time series at country level, we should update the individual gridcells values accordingly.\n",
    "To do so, we apply the same gridcells weights as in the original population dataset to perform the spatial downscaling of the new national time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11e8414e-5601-4598-acbe-8df5252f91b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data_with_corrected_national_time_series = hist_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df154cab-926f-4b4c-bfb2-bd18a5d67f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year completed: 1880\n",
      "Year completed: 1900\n",
      "Year completed: 1920\n",
      "Year completed: 1940\n",
      "Year completed: 1960\n",
      "Year completed: 1980\n",
      "Year completed: 2000\n",
      "Year completed: 2020\n",
      "Year completed: 2040\n"
     ]
    }
   ],
   "source": [
    "downscaling_coefficients = hist_data_with_corrected_national_time_series[0, :, :].copy(deep=True)*0.0\n",
    "corrected_data_for_given_year = hist_data_with_corrected_national_time_series[:, :, :].copy(deep=True)*0.0 \n",
    "mask_with_total_corrected_countries_population = hist_data_with_corrected_national_time_series[0, :, :].copy(deep=True)*0.0 \n",
    "array_ones = hist_data_with_corrected_national_time_series[0, :, :].copy(deep=True)*0.0 +1.0 \n",
    "\n",
    "# Iterate over each year and country\n",
    "for year in range(1861, 2101):\n",
    "    downscaling_coefficients *= 0.0\n",
    "    mask_with_total_corrected_countries_population *=0.0\n",
    "    \n",
    "    for country in range(len(countries_index)):\n",
    "        # Calculate country total population for given year\n",
    "        corrected_country_total_population_given_year = np.nansum(corrected_country_pop_timeseries[country, year-1861].where(mask == country))\n",
    "        \n",
    "        original_country_total_population_given_year = np.nansum(country_pop_timeseries[country, year-1861].where(mask == country))\n",
    "        downscaling_coefficients = downscaling_coefficients + (hist_data[year-1861].where(mask == country)/original_country_total_population_given_year).fillna(0) # fillna(0) used to avoid summation with NaN values which would occur because using masking for second term of the summation\n",
    "\n",
    "        # Save total corrected country population value in each gridcell corresponding to this country\n",
    "        mask_with_total_corrected_countries_population = mask_with_total_corrected_countries_population + (array_ones.where(mask == country)*corrected_country_total_population_given_year).fillna(0)\n",
    "        \n",
    "        if (country != 23 and country !=159): # if country different from Fr. S. Antarctic Lands and Antarctica where population is 0\n",
    "            # Check if total countries' population was saved correctly in each gridcell corresponding to that country\n",
    "            # We do this by: [[sum over all grid points in mask_with_total_countries_population corresponding to given country]/[number of grid points part of given country]]/[country total population for given year obtained from scenario dataset] - 1 \n",
    "            # If the procedure was performed correctly, this operation should give 0 within certain numerical error (here acceptable limit 10e-6)  \n",
    "            if np.abs(np.nansum(mask_with_total_corrected_countries_population.where(mask==country).values)/np.nansum(np.isnan(mask_with_total_corrected_countries_population.where(mask==country).values) == False)/corrected_country_total_population_given_year - 1) > 10e-6:\n",
    "                print(\"ERROR: it seems that country total population was not saved correctly in each gridcell.\")\n",
    "                print(\"This will cause incorect downscaling.\")\n",
    "                print(\"Please check the code for potential issues.\")\n",
    "                print(\"The error message concerns: \" + str(countries_names[country]))\n",
    "                print(\"For assistance, if required, please contact sabin.taranu@vub.be\")\n",
    "        \n",
    "    # Here we assemble the gridded corrected population data for given year as the sum of downscaled total countries' population for given year (correction achieved for 177 countries + 1 fictive country corresponding to gridcells outside the regionmask countries)\n",
    "    corrected_data_for_given_year[year-1861] = corrected_data_for_given_year[year-1861] + mask_with_total_corrected_countries_population*downscaling_coefficients\n",
    "    # Return 0 population values into NaNs:\n",
    "    corrected_data_for_given_year[year-1861] = corrected_data_for_given_year[year-1861].where(corrected_data_for_given_year[year-1861] != 0)\n",
    "    \n",
    "    if year%20 == 0:\n",
    "        print(\"Year completed: \" + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099c8d6-cc80-43aa-b7a9-3b481bf917fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new data\n",
    "corrected_data_for_given_year.to_netcdf(path = \"../corrected/population_with_corrected_countries_time_series_1861_2100.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211f194-cafb-4557-8a76-d6eee4d3d340",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Check that our correction was successful, and that the new gridded population dataset when aggregated at country level is fully matching the corrected_country_pop_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc84324-ddf9-43c4-a751-bbafe33317f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a zero array with countries and time dimensions\n",
    "country_pop_timeseries_aggregated_from_the_country_time_series_corrected_dataset = xr.DataArray(np.zeros((178, 240)), dims=('country', 'time'), \n",
    "                                      coords={'country': countries_index, 'time': range(1861, 2101)})\n",
    "\n",
    "# Loop over years:\n",
    "for year in range(1861, 2101):\n",
    "    # Loop over each countries:\n",
    "    for country in range(len(countries_index)):\n",
    "        # Calculate country total population for given year\n",
    "        country_pop_timeseries_aggregated_from_the_country_time_series_corrected_dataset[country, year-1861] = np.nansum(corrected_data_for_given_year[year-1861].where(mask == country))\n",
    "        \n",
    "    if year%20==0:\n",
    "        print(\"Year completed: \" + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85378b02-4fdb-41e2-82ec-59a476a5fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.abs(np.nansum(country_pop_timeseries_aggregated_from_the_country_time_series_corrected_dataset-corrected_country_pop_timeseries)) > 10e-8*(np.max(corrected_country_pop_timeseries.sum('country'))):\n",
    "    print(\"It seems an error was produced during the creation of the gridded dataset with corrected countries time series, as the difference between corrected countries time series and re-aggregated time series is larger than expected numerical error limit.\")\n",
    "    print(\"np.nansum(country_pop_timeseries_aggregated_from_the_country_time_series_corrected_dataset-corrected_country_pop_timeseries) = \" + str(np.nansum(country_pop_timeseries_aggregated_from_the_country_time_series_corrected_dataset-corrected_country_pop_timeseries)))\n",
    "    print(\"Please check the code for possible issue, or request assistance from sabin.taranu@vub.be if needed.\")\n",
    "else:\n",
    "    print(\"The creation of the new gridded dataset with corrected countries time series was succesful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99770283-f870-45fd-9d37-874962e31623",
   "metadata": {},
   "source": [
    "# Check global population before and after correction:\n",
    "The results show that:\n",
    "1) No correction was done till 2005.\n",
    "2) The total contribution of the correction was an increase in the total global population by about 4-5 millions. This is insignificant change, which do not change drastically the nature of the scenario, at the same time ensuring high quality of individual countries time series\n",
    "3) The offset is larger for the period 2005-2010 because this where linear interpolation was used in the original dataset, and where smoothing was used by us for some countries time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905329a9-0889-45d0-ba50-2d0a71a6b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pop_original = country_pop_timeseries.sum('country')\n",
    "global_pop_after_correcting_national_time_series = corrected_country_pop_timeseries.sum('country')\n",
    "\n",
    "# Assuming that your time array looks something like this:\n",
    "time = range(1861, 2101)\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the original global population with red circles\n",
    "plt.plot(time, global_pop_original-global_pop_after_correcting_national_time_series, 'k-', label='Difference between original global population and one after correcting countries time series')\n",
    "\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Global Population from 1861 to 2100')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Global Population')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae7189f-e9a5-4c9b-bb02-3b9194fe0d98",
   "metadata": {},
   "source": [
    "# Spatial consistency of the dataset:\n",
    "Since we are now sure that national time series are correct for each country, we also need to check how this national population is distributed across gridcells.\n",
    "For example, Inne Valderkelen identified that there is a spatial inconsistency at the transition year 2005-2006, between the historic and ssp2 dataset.\n",
    "The spatial inconsistency consist in the fact, that some gridcell time series experience a sudden drop or jump at the transition year (e.g. from 0 to 100000 people in one year).\n",
    "This is an unrealistic behavior, making it impossible to use for analysis on gridcell level, and putting under question model results which were generated using this data.\n",
    "\n",
    "While a solution was already proposed for this specific issue, it is worth looking if there are no similar issues for the remaining of years.\n",
    "Also, to be more precise in our assessment of the spatial consistency, we will analyze this property at country level instead of global.\n",
    "\n",
    "To do this, we need an objective metric to estimate the spatial consistency between each consecutive years.\n",
    "For this scope, we propose the following metric:\n",
    "\n",
    "metric_spatial_consistency_for_given_country_and_transition_year(N-1) =  Sum_over_all_grids[absolute_value(gridded_country_pop_year(N) - gridded_country_pop_year(N-1))]/[absolute_value(aggregated_country_pop_year(N) - aggregated_country_pop_year(N-1))] \n",
    "\n",
    "Normally, we can expect the following behavior from this metric:\n",
    "1) If the weights of each gridcell for a given country, remain the same at the transition between the year N and N-1, then this metric will return 1. \n",
    "2) If the weights are changing, it means, that at the transition between year N and N-1, some grids gain population weight relatively to others in the same country. In this case the metric will return a value larger than 1. \n",
    "\n",
    "It should be mentioned that in practice, it is ok to have this metric > 1. Actually in reality, most of the time this will be the case, as human settlements in a given country can grow at different rates, which gradually is changing the grids weights with some places cumulating population faster or slower.\n",
    "\n",
    "At the same time, if we observe an anomaly in this metric, with a sudden change significantly exceeding the normal statistics, it is likely due to a problem with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e8c0f8-9952-4b66-8e35-4e9b1bcda527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a zero array with countries and time dimensions\n",
    "metric_spatial_consistency_for_given_country_and_transition_year = xr.DataArray(np.zeros((178, 239)), dims=('country', 'time'), \n",
    "                                                                  coords={'country': countries_index, 'time': range(1861, 2100)})\n",
    "\n",
    "# Loop over years:\n",
    "for year in range(1861, 2100):\n",
    "    # Loop over each countries:\n",
    "    for country in range(len(countries_index)):\n",
    "        # Calculate country total population for given year\n",
    "        metric_spatial_consistency_for_given_country_and_transition_year[country, year-1861] = np.nansum(np.abs(corrected_data_for_given_year[year-1861+1].where(mask==country) - corrected_data_for_given_year[year-1861].where(mask==country)))/(np.abs(np.nansum(corrected_data_for_given_year[year-1861+1].where(mask==country)) - np.nansum(corrected_data_for_given_year[year-1861].where(mask==country))))\n",
    "\n",
    "    if year%20==0:\n",
    "        print(\"Year completed: \" + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba40ea-9818-4097-847a-ca1c6707d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming time series starts from 1861 and ends in 2100\n",
    "years = pd.date_range(start='1861', periods=239, freq='Y').year\n",
    "\n",
    "# Iterate over each country and its population time series\n",
    "for i, country in enumerate(countries_names):\n",
    "    country_spatial_consistency = metric_spatial_consistency_for_given_country_and_transition_year[i] \n",
    "    \n",
    "    # Creating a DataFrame for easy manipulation\n",
    "    df = pd.DataFrame({'Year': years, 'Country_spatial_consistency': country_spatial_consistency})\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['Year'], df['Country_spatial_consistency'], 'k-', markersize=3)\n",
    "    plt.axvline(x=2005, color='green', linestyle='--')  # This line adds a vertical line at the year 2005\n",
    "    plt.axvline(x=2010, color='green', linestyle='--')  # This line adds a vertical line at the year 2005\n",
    "    plt.title(country)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Spatial Consistency Metric')\n",
    "    \n",
    "    # Save to pdf\n",
    "    # pdf_path = f\"../results/country_level_spatial_consistency/{country}_spatial_consistency_metric_1861_2100.pdf\"\n",
    "    pdf_path = f\"../results_to_share/{country}_spatial_consistency_metric_1861_2100.pdf\"\n",
    "\n",
    "    plt.savefig(pdf_path, format='pdf')\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060b46c-c0ca-4a99-b2c0-e39ee45287b4",
   "metadata": {},
   "source": [
    "# Understand normal vs spatial inconsistent behavior\n",
    "As we mentioned, it is normal that sometimes the metric we compute exceeds 1.0.\n",
    "This can correspond to a situation where population in some grids is changing faster than the others for the same country.\n",
    "\n",
    "At the same time, we know that the years 2005-2010 are anomalous for the dataset, as spacial inconsistency is introduced through merging the historic and ssp data. Therefore, we can try to use this period to understand what an abnormal value for our metric is.\n",
    "\n",
    "By collecting the statistics over the period 1861-2004 and 2005-2010 for all countries for our spatial inconsistency metric, we can build density functions. By comparing the two density functions, we can then understand what is the likely threshold over which, maybe, there is a spatial inconsistency situation.\n",
    "\n",
    "We can see that the period from 2011-2100, while mostly spatially consistent, there are some signs of spatial consistency anomalies.\n",
    "This can also be confirmed by looking at the individual countries time series of the spatial consistency metrics (computed and saved in the previous step). \n",
    "In some sense, this raises the question of why this is happening (the assumptions behind the scenario data creation) and to what extent we can trust the robustness and continuity of individual gridcell time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a171a-6dbc-4734-806f-5b9c96265d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# assuming the xarray DataArray is already loaded as metric_spatial_consistency_for_given_country_and_transition_year\n",
    "\n",
    "# Subset the data for the three periods\n",
    "period1_data = metric_spatial_consistency_for_given_country_and_transition_year.sel(time=slice(1861,2004))\n",
    "period2_data = metric_spatial_consistency_for_given_country_and_transition_year.sel(time=slice(2005,2010))\n",
    "period3_data = metric_spatial_consistency_for_given_country_and_transition_year.sel(time=slice(2011,2100))\n",
    "\n",
    "# Reshape data to 1D array for the KDE function\n",
    "period1_data_1d = period1_data.values.flatten()\n",
    "period2_data_1d = period2_data.values.flatten()\n",
    "period3_data_1d = period3_data.values.flatten()\n",
    "\n",
    "# Remove NaN and Inf values\n",
    "period1_data_1d = period1_data_1d[~np.isnan(period1_data_1d) & ~np.isinf(period1_data_1d)]\n",
    "period2_data_1d = period2_data_1d[~np.isnan(period2_data_1d) & ~np.isinf(period2_data_1d)]\n",
    "period3_data_1d = period3_data_1d[~np.isnan(period3_data_1d) & ~np.isinf(period3_data_1d)]\n",
    "\n",
    "# Replace values larger than 100 with 100\n",
    "period1_data_1d = np.where(period1_data_1d > 100, 100, period1_data_1d)\n",
    "period2_data_1d = np.where(period2_data_1d > 100, 100, period2_data_1d)\n",
    "period3_data_1d = np.where(period3_data_1d > 100, 100, period3_data_1d)\n",
    "\n",
    "# Generate kernel density estimates for the three periods\n",
    "kde1 = gaussian_kde(period1_data_1d)\n",
    "kde2 = gaussian_kde(period2_data_1d)\n",
    "kde3 = gaussian_kde(period3_data_1d)\n",
    "\n",
    "# Define a range for the x-axis (you might want to adjust this based on your data)\n",
    "x_range = np.linspace(min(min(period1_data_1d), min(period2_data_1d), min(period3_data_1d)), max(max(period1_data_1d), max(period2_data_1d), max(period3_data_1d)), 500)\n",
    "\n",
    "# Evaluate the KDE functions on the x_range\n",
    "kde1_eval = kde1.evaluate(x_range)\n",
    "kde2_eval = kde2.evaluate(x_range)\n",
    "kde3_eval = kde3.evaluate(x_range)\n",
    "\n",
    "# Generate the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_range, kde1_eval, label='1861-2004', color='blue')\n",
    "plt.plot(x_range, kde2_eval, label='2005-2010', color='red')\n",
    "plt.plot(x_range, kde3_eval, label='2011-2100', color='green')\n",
    "plt.title('Probability Density Functions')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim([0,20])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ddb05b-1fca-45f5-9237-d8f1addb2c40",
   "metadata": {},
   "source": [
    "# What is the threshold?\n",
    "A good estimate can be the moment when kde2_eval > kde1_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daa4112-1ae4-4b48-adde-9068bd3caa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_spatial_inconsistency = x_range[kde2_eval > kde1_eval][0]\n",
    "print(threshold_spatial_inconsistency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5797c12c-380d-445f-b4ef-217f191af783",
   "metadata": {},
   "source": [
    "# Solve the spatial inconsistency issue for 2005-2100\n",
    "Here we use the same method as in the previous correction (already accepted and communicated in a caveat for the ISIMIP2), with the exception that now we also created the fictive country \"Not a regionmask country\" (index 177), which helps achieve spatial consistency also for insular countries and coastline gridcells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edaa353-4fbb-4263-b86f-9f59fbbb4aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year 2005 data from the reference population_histsoc_0p5deg_annual_1861-2005.nc4\n",
    "# We want to achieve spatial consistency with this dataset for all future years in the scenarios data\n",
    "ref_pop_2005 = xr.open_dataset(\"../corrected/population_with_corrected_countries_time_series_1861_2100.nc\", engine='h5netcdf', decode_times=False).pop[144,:,:]\n",
    "\n",
    "# Load the scenario population dataset we want to correct:\n",
    "pop_future = xr.open_dataset(\"../corrected/population_with_corrected_countries_time_series_1861_2100.nc\", engine='h5netcdf', decode_times=False).pop[145:,:,:]\n",
    "\n",
    "# Generate a mask object for each country based on the grid of our input data\n",
    "# This will result in an object that for a given country will have all corresponding gridcells filled with the country index value\n",
    "# For example for US all corresponding gridcells will be filled with \"4\"\n",
    "# All gridcells which are not part of any country will be filled with NaN\n",
    "mask = countries.mask(ref_pop_2005).fillna(177) # we fillna values with a custom key \"177\" because some of these points not identified as countries due to limits of regionmask, actually contain some population (e.g., an insular country or not atributed coastline gridcell) and using this key we can still have access to these points\n",
    "\n",
    "# Obtain a list of all countries names and index:\n",
    "countries_names = countries.names\n",
    "countries_index = countries.numbers\n",
    "\n",
    "# Create a fictive country which would contain all the 999 masked gridcells\n",
    "countries_names.append(\"Not a regionmask country\")\n",
    "countries_index.append(177)\n",
    "\n",
    "# Get number of years in the future dataset to loop over\n",
    "years = pop_future.shape[0]\n",
    "\n",
    "# Destination to save file:\n",
    "destination = \"../corrected/\"\n",
    "\n",
    "# Name of corrected dataset:\n",
    "file_name = \"corrected_population_ssp2soc_0p5deg_annual_2006-2100.nc\"\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61f64c-67d9-4412-8f66-2b23258e45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the correction indices\n",
    "# The meaning of this dataset for a given country is: what fraction of the given country total population is located in a given gridcell\n",
    "# For example if we mask for US (index 4), and we find that one gridcell in correction_indices for US contains the value 0.05,\n",
    "# it means that this gridcell contains 5% of total US population\n",
    "# If we want to obtain spatially consistent data at the transition between 2005 and 2006, it means that for each country, these fraction coefficients should stay the same during the transition and for future years\n",
    "correction_indices = ref_pop_2005.copy(deep = True)*0.0 # initialize each gridcell with dummy data (zeros)\n",
    "\n",
    "# Loop over each country\n",
    "for country in range(len(countries_index)):\n",
    "    # Calculate country total population\n",
    "    country_total_population_ref = np.nansum(ref_pop_2005.where(mask == country))\n",
    "    # Compute what is the fraction of contribution of each country gridcell to its total population for the reference year 2005\n",
    "    correction_indices = correction_indices + (ref_pop_2005.where(mask == country)/country_total_population_ref).fillna(0) # fillna(0) used to avoid summation with NaN values which would occur because using masking for second term of the summation\n",
    "\n",
    "    # Perform a series of tests to detect potential issues:\n",
    "    # In principle, we expect the sum over all fractions to return 1\n",
    "    # If it returns 0, it means that the algorithm detected no population\n",
    "    # After running the script, we find that 0.0 is returned only for Fr. S. Antarctic Lands and Antarctica (which make sense)\n",
    "    # For all remaining countries, sum over all fractions equal 1 \n",
    "    if np.nansum(correction_indices.where(mask==country).values) == 0.0:\n",
    "        print(\"Attention: the sum over all contribution coefficients is 0.0, meaning that there is no population.\")\n",
    "        print(\"This happens for the country: \" + str(countries_names[country]))\n",
    "        print(\"Country index is: \" + str(country))\n",
    "        print(\"If this does not make sense for this country, please check the code!\")\n",
    "        print(\"For assistance, if needed, please contact sabin.taranu@vub.be.\")\n",
    "    if (np.abs(np.nansum(correction_indices.where(mask==country).values) - 1) > 10e-6) and (np.nansum(correction_indices.where(mask==country).values) != 0.0): # acceptable numerical error limit 10e-6\n",
    "        print(\"ERROR: the sum over all contribution coefficients do not add to 1.0\")\n",
    "        print(\"Problem for the country: \" + str(countries_names[country]))\n",
    "        print(\"Country index is: \" + str(country))\n",
    "        print(\"Coefficients sum is: \" + str(np.nansum(correction_indices.where(mask==country).values)))\n",
    "        print(\"For assistance, if needed, please contact sabin.taranu@vub.be.\")\n",
    "        break\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f61ed-fea2-48f1-a1ef-86834e1bd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_data = pop_future.copy(deep=True)*0.0 # prepare array to store corrected data for each year in the scenario dataset (initialize with dummy values - zeros)\n",
    "array_ones = ref_pop_2005.copy(deep = True)*0.0+1.0 # xarray of ones with same shape as ref_pop_2005; we will use this array for basic operations in the following \n",
    "\n",
    "# Loop over all future years:\n",
    "for year in range(years):\n",
    "    # previously we introduced the mask object, which had a given index value in each gridcell identified to be part of a given country\n",
    "    # we now want to create a similar mask object, but instead of corresponding index value, it will have total country population in each gridcell of given country\n",
    "    # we will use this array, together with correction indices, to perform spatial downscalling of future countries populations with consistent spatial distribution with year 2005 of the ref dataset\n",
    "    mask_with_total_countries_population = ref_pop_2005.copy(deep=True)*0.0 # initialize with dummy data (zeros)\n",
    "    # Loop over each country\n",
    "    for country in range(len(countries_index)):\n",
    "        # Calculate country total population for given year\n",
    "        country_total_population_given_year = pop_future[year].where(mask == country).sum()\n",
    "        # Save total country population value in each gridcell corresponding to this country\n",
    "        mask_with_total_countries_population = mask_with_total_countries_population + (array_ones.where(mask == country)*country_total_population_given_year).fillna(0)\n",
    "        \n",
    "        if (country != 23 and country !=159): # if coutry different from Fr. S. Antarctic Lands and Antarctica where population is 0\n",
    "            # Check if total countries population was saved correctly in each gridcell corresponding to that country\n",
    "            # We do this by: [[sum over all grid points in mask_with_total_countries_population corresponding to given country]/[number of grid points part of given country]]/[country total population for given year obtained from scenario dataset] - 1 \n",
    "            # If the procedure was performed correctly, this operation should give 0 within certain numerical error (here acceptable limit 10e-6)  \n",
    "            if np.abs(np.nansum(mask_with_total_countries_population.where(mask==country).values)/np.nansum(np.isnan(mask_with_total_countries_population.where(mask==country).values) == False)/country_total_population_given_year - 1) > 10e-6:\n",
    "                print(\"ERROR: it seems that country total population was not saved correctly in each gridcell.\")\n",
    "                print(\"This will cause incorect downscaling.\")\n",
    "                print(\"Please check the code for potential issues.\")\n",
    "                print(\"The error message concerns: \" + str)\n",
    "                print(\"For assistance, if required, please contact sabin.taranu@vub.be\")\n",
    "    \n",
    "    # Normally at this stage correction was achieved for all gridcells which are within the regionmask 177 countries borders.\n",
    "    # The problem is that there are still some gridcells which will have a strong transition bias because they are on gridcells which were not masked with regionmask countries definitation (e.g. small insular countries, or some coastline grids)\n",
    "    # To deal with this problem, we treat it in a similar way to the country correction algorithm\n",
    "    # First, we suppose that all grids outside the 177 regionmask countries, are part of an imaginary non_regionmask_global_country\n",
    "    # Then we can apply the same correction principle, by calculating how much people live in this non_regionmask_global_country and apply the downscaling coefficients we learn from year 2005\n",
    "        \n",
    "    # Here we assemble the gridded corrected population data for given year as the sum of downscaled total countries population for given year (correction achieved for 177 countries + 1 fictive country corresponding to gridcells outside the regionmask countries)\n",
    "    corrected_data[year] = corrected_data[year] + mask_with_total_countries_population*correction_indices\n",
    "    # Return 0 population values into NaNs:\n",
    "    corrected_data[year] = corrected_data[year].where(corrected_data[year] != 0)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3226a3-acc1-4fe1-9d81-1b46f41cf293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the corrected data\n",
    "corrected_data.to_netcdf(path = destination+file_name)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f4d7f-d17f-470d-861f-4ccf7c5544c2",
   "metadata": {},
   "source": [
    "# Visualize the problem as well as added value of the solution:\n",
    "In the following we perform some basic plots of population data at the transion between year 2005 and 2006.\n",
    "While spatial inconsistencies can already be seen with naked eye on the global maps between year 2005 and 2006 for original data, we also add two regionals maps (Australia and Northern Africa) with black boxes around regions with prominent spatial discrepancies.\n",
    "\n",
    "The reader can also notice that these spatial inconsistencies disapeared in the corrected verision of population data for year 2006.\n",
    "In the following we will provide a more complete validation of our correction strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaef3ae-1471-4f3c-bd6b-28098e60e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_pop_2005 = xr.open_dataset(\"../data/population_histsoc_rcp26soc_0p5deg_annual_1861-2100.nc4\", engine='h5netcdf', decode_times=False).pop[144,:,:]\n",
    "original_pop_2006 = xr.open_dataset(\"../data/population_histsoc_rcp26soc_0p5deg_annual_1861-2100.nc4\", engine='h5netcdf', decode_times=False).pop[145,:,:]\n",
    "corrected_pop_2006 = xr.open_dataset(\"../corrected/corrected_population_ssp2soc_0p5deg_annual_2006-2100.nc\", engine='h5netcdf', decode_times=False).pop[0,:,:]\n",
    "\n",
    "\n",
    "mask_all_countries = (np.isnan(countries.mask(hist_pop_2005)) == False)\n",
    "\n",
    "lon = hist_pop_2005.lon\n",
    "lat = hist_pop_2005.lat\n",
    "\n",
    "# Prepare projection\n",
    "data_crs = ccrs.PlateCarree()\n",
    "projection = ccrs.PlateCarree()\n",
    "# Prepare colormap\n",
    "cmap = plt.cm.Reds\n",
    "\n",
    "fig, axarr = plt.subplots(nrows=3, ncols=3, figsize=(15*3, 5*3), constrained_layout=False, subplot_kw={'projection': projection})\n",
    "axlist = axarr.flatten()\n",
    "\n",
    "for ax in axlist:\n",
    "    ax.coastlines()\n",
    "\n",
    "\n",
    "\n",
    "map1 = axlist[0].pcolormesh(lon, lat, hist_pop_2005.where(mask_all_countries == True), norm=colors.LogNorm(vmin=1, vmax=np.nanmax(hist_pop_2005.where(mask_all_countries == True))), cmap=cmap, shading='auto', transform=ccrs.PlateCarree())\n",
    "axlist[0].set_title('(a) Population Year 2005 \\n(HIST)', fontsize=16)\n",
    "\n",
    "map2 = axlist[1].pcolormesh(lon, lat, original_pop_2006.where(mask_all_countries == True), norm=colors.LogNorm(vmin=1, vmax=np.nanmax(hist_pop_2005.where(mask_all_countries == True))), cmap=cmap, shading='auto', transform=ccrs.PlateCarree())\n",
    "axlist[1].set_title('(b) Population year 2006 \\n(Original)', fontsize=16)\n",
    "\n",
    "map3 = axlist[2].pcolormesh(lon, lat, corrected_pop_2006.where(mask_all_countries == True), norm=colors.LogNorm(vmin=1, vmax=np.nanmax(hist_pop_2005.where(mask_all_countries == True))), cmap=cmap, shading='auto', transform=ccrs.PlateCarree())\n",
    "axlist[2].set_title('(c) Population year 2006 \\n(Corrected)', fontsize=16)\n",
    "\n",
    "\n",
    "map4 = axlist[3].pcolormesh(lon, lat, hist_pop_2005.where(mask_all_countries == True), norm=colors.LogNorm(vmin=1, vmax=np.nanmax(hist_pop_2005.where(mask_all_countries == True))), cmap=cmap, shading='auto', transform=ccrs.PlateCarree())\n",
    "axlist[3].set_extent([105, 157, -44.2, -9.3], crs=data_crs)\n",
    "axlist[3].set_title('(d) Population Year 2005 \\n(HIST, zoom over Australia)', fontsize=16)\n",
    "\n",
    "map5 = axlist[4].pcolormesh(lon, lat, original_pop_2006.where(mask_all_countries == True), norm=colors.LogNorm(vmin=1, vmax=np.nanmax(hist_pop_2005.where(mask_all_countries == True))), cmap=cmap, shading='auto', transform=ccrs.PlateCarree())\n",
    "axlist[4].set_extent([105, 157, -44.2, -9.3], crs=data_crs)\n",
    "axlist[4].set_title('(e) Population year 2006 \\n(Original, zoom over Australia)', fontsize=16)\n",
    "\n",
    "map6 = axlist[5].pcolormesh(lon, lat, corrected_pop_2006.where(mask_all_countries == True), norm=colors.LogNorm(vmin=1, vmax=np.nanmax(hist_pop_2005.where(mask_all_countries == True))), cmap=cmap, shading='auto', transform=ccrs.PlateCarree())\n",
    "axlist[5].set_extent([105, 157, -44.2, -9.3], crs=data_crs)\n",
    "axlist[5].set_title('(f) Population year 2006 \\n(Corrected, zoom over Australia)', fontsize=16)\n",
    "\n",
    "\n",
    "map7 = axlist[6].pcolormesh(lon, lat, hist_pop_2005.where(mask_all_countries == True), norm=colors.LogNorm(vmin=1, vmax=np.nanmax(hist_pop_2005.where(mask_all_countries == True))), cmap=cmap, shading='auto', transform=ccrs.PlateCarree())\n",
    "axlist[6].set_extent([-23.6, 59.7, 4, 37.7], crs=data_crs)\n",
    "axlist[6].set_title('(g) Population Year 2005 \\n(HIST, zoom over Northern Africa)', fontsize=16)\n",
    "\n",
    "map8 = axlist[7].pcolormesh(lon, lat, original_pop_2006.where(mask_all_countries == True), norm=colors.LogNorm(vmin=1, vmax=np.nanmax(hist_pop_2005.where(mask_all_countries == True))), cmap=cmap, shading='auto', transform=ccrs.PlateCarree())\n",
    "axlist[7].set_extent([-23.6, 59.7, 4, 37.7], crs=data_crs)\n",
    "axlist[7].set_title('(h) Population year 2006 \\n(Original, zoom over Northern Africa)', fontsize=16)\n",
    "\n",
    "map9 = axlist[8].pcolormesh(lon, lat, corrected_pop_2006.where(mask_all_countries == True), norm=colors.LogNorm(vmin=1, vmax=np.nanmax(hist_pop_2005.where(mask_all_countries == True))), cmap=cmap, shading='auto', transform=ccrs.PlateCarree())\n",
    "axlist[8].set_extent([-23.6, 59.7, 4, 37.7], crs=data_crs)\n",
    "axlist[8].set_title('(i) Population year 2006 \\n(Corrected, zoom over Northern Africa)', fontsize=16)\n",
    "\n",
    "#plt.subplots_adjust(wspace=0.05, hspace=0.15)\n",
    "#plt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=0, hspace=0)\n",
    "\n",
    "axlist[4].add_patch(patches.Rectangle(\n",
    "        #xy=(20, 15),  # point of origin.\n",
    "       xy=(118, -33),\n",
    "       width=12, height=4, linewidth=3,\n",
    "       color='black', fill=False))\n",
    "\n",
    "axlist[7].add_patch(patches.Rectangle(\n",
    "       xy=(-14, 15),  # point of origin.\n",
    "       #xy=(118, -33),\n",
    "       width=70, height=20, linewidth=3,\n",
    "       color='black', fill=False))\n",
    "\n",
    "cb = fig.colorbar(mpl.cm.ScalarMappable(norm=colors.LogNorm(vmin=1, vmax=np.nanmax(hist_pop_2005.where(mask_all_countries == True))), cmap=cmap), ax=axlist.ravel().tolist(), orientation='vertical')\n",
    "cb.set_label('People', size=16, weight='bold', rotation=0, y=1.05)\n",
    "cb.ax.tick_params(labelsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d01b46-da50-4a34-9c48-25852e8e4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import xarray as xr\n",
    "\n",
    "# Create the relative difference DataArrays and replace inf values\n",
    "rel_diff_orig = ((original_pop_2006 - hist_pop_2005) / hist_pop_2005) *100\n",
    "rel_diff_orig = xr.where(np.isinf(rel_diff_orig), np.nan, rel_diff_orig)\n",
    "\n",
    "rel_diff_corr = ((corrected_pop_2006 - hist_pop_2005) / hist_pop_2005) *100\n",
    "rel_diff_corr = xr.where(np.isinf(rel_diff_corr), np.nan, rel_diff_corr)\n",
    "\n",
    "# Define the continental extents\n",
    "extents = {'global': [-180, 180, -60, 90], # Avoiding Antarctica\n",
    "           'North America': [-170, -50, 10, 85],\n",
    "           'South America': [-90, -30, -60, 15],\n",
    "           'Europe': [-25, 45, 35, 85],\n",
    "           'Africa': [-20, 55, -35, 40],\n",
    "           'Asia': [45, 180, 5, 85],\n",
    "           'Australia': [113, 154, -44, -10]}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=7, ncols=2, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(15, 30))\n",
    "\n",
    "for row, extent in enumerate(extents.values()):\n",
    "    for col, data in enumerate([rel_diff_orig, rel_diff_corr]):\n",
    "        ax = axes[row, col]\n",
    "        ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "        ax.coastlines()\n",
    "\n",
    "        # Plot the data\n",
    "        im = data.plot(ax=ax, transform=ccrs.PlateCarree(), add_colorbar=False, cmap='RdBu_r', vmin=-100, vmax=100)\n",
    "\n",
    "        # Add a colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax, orientation='vertical', pad=0.03)\n",
    "        cbar.set_label(\"Relative difference\", size=14)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf18b5-73a1-4655-b437-9693d5bd3d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5770d4-d700-48b5-893a-e3bb7b6e4f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
